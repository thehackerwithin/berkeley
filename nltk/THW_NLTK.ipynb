{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's talk will address various concepts in the Natural Language Processing pipeline through the use of NLTK. A fundmental understanding of Python is necessary. We will cover:\n",
    "\n",
    "1. Pre-processing\n",
    "2. Preparing and declaring your own corpus\n",
    "3. POS-Tagging\n",
    "4. Dependency Parsing\n",
    "5. NER\n",
    "6. Sentiment Analysis\n",
    "\n",
    "\n",
    "You will need:\n",
    "\n",
    "* NLTK ( \\$ pip install nltk)\n",
    "* the parser wrapper requires the [Stanford Parser](http://nlp.stanford.edu/software/lex-parser.shtml#Download) (in Java)\n",
    "* the NER wrapper requires the [Stanford NER](http://nlp.stanford.edu/software/CRF-NER.shtml#Download) (in Java)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This won't be covered much today, but regex and basic python string methods are most important in preprocessing tasks. NLTK does, however, offer an array of tokenizers and stemmers for various languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = '''Hello, my name is Chris. \n",
    "I'll be talking about the python library NLTK today. \n",
    "NLTK is a popular tool to conduct text processing tasks in NLP.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"Notice the difference!\")\n",
    "print()\n",
    "print(word_tokenize(text))\n",
    "\n",
    "print()\n",
    "print(\"vs.\")\n",
    "print()\n",
    "\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also tokenize sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of sentences with a list of tokenized words is generally the accepted format for most libraries for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming/Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "print(snowball.stem('running'))\n",
    "print(snowball.stem('eats'))\n",
    "print(snowball.stem('embarassed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But watch out for errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(snowball.stem('cylinder'))\n",
    "print(snowball.stem('cylindrical'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or collision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(snowball.stem('vacation'))\n",
    "print(snowball.stem('vacate'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why lemmatizing, if the computing power and time is sufficient, is always preferable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "print(wordnet.lemmatize('vacation'))\n",
    "print(wordnet.lemmatize('vacate'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why is this important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "categories = ['talk.politics.mideast', 'rec.autos', 'sci.med']\n",
    "twenty = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "data_no_stems = [[word_tokenize(sent) for sent in sent_tokenize(text.lower())] for text in twenty.data]\n",
    "data_stems = [[[snowball.stem(word) for word in word_tokenize(sent)]\n",
    "               for sent in sent_tokenize(text)] for text in twenty.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data_no_stems[400][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data_stems[400][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_no_stems = [' '.join([item for sublist in l for item in sublist]) for l in data_no_stems]\n",
    "data_stems = [' '.join([item for sublist in l for item in sublist]) for l in data_stems]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_data_no_stems = vectorizer.fit_transform(data_no_stems)\n",
    "\n",
    "vectorizer2 = TfidfVectorizer()\n",
    "X_data_stems = vectorizer2.fit_transform(data_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import ensemble\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data_no_stems, twenty.target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "rf_classifier = ensemble.RandomForestClassifier(n_estimators=10,  # number of trees\n",
    "                       criterion='gini',  # or 'entropy' for information gain\n",
    "                       max_depth=None,  # how deep tree nodes can go\n",
    "                       min_samples_split=2,  # samples needed to split node\n",
    "                       min_samples_leaf=1,  # samples needed for a leaf\n",
    "                       min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                       max_features='auto',  # number of features for best split\n",
    "                       max_leaf_nodes=None,  # max nodes\n",
    "                       min_impurity_split=1e-07,  # early stopping\n",
    "                       n_jobs=1,  # CPUs to use\n",
    "                       class_weight=\"balanced\")  # adjusts weights inverse of freq, also \"balanced_subsample\" or None\n",
    "\n",
    "model = rf_classifier.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data_stems, twenty.target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "rf_classifier = ensemble.RandomForestClassifier(n_estimators=10,  # number of trees\n",
    "                       criterion='gini',  # or 'entropy' for information gain\n",
    "                       max_depth=None,  # how deep tree nodes can go\n",
    "                       min_samples_split=2,  # samples needed to split node\n",
    "                       min_samples_leaf=1,  # samples needed for a leaf\n",
    "                       min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                       max_features='auto',  # number of features for best split\n",
    "                       max_leaf_nodes=None,  # max nodes\n",
    "                       min_impurity_split=1e-07,  # early stopping\n",
    "                       n_jobs=1,  # CPUs to use\n",
    "                       class_weight=\"balanced\")  # adjusts weights inverse of freq, also \"balanced_subsample\" or None\n",
    "\n",
    "model = rf_classifier.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Declaring a corpus in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you can use NLTK on strings and lists of sentences, it's better to formally declare your corpus, as this will take care of the above for you and provide methods to access them. For our purposes today, we'll use a corpus of [book summaries](http://www.cs.cmu.edu/~dbamman/booksummaries.html). I've changed them into a folder of .txt files for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "corpus_root = \"texts/\"  # relative path to texts.\n",
    "my_texts = PlaintextCorpusReader(corpus_root, '.*txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a text corpus, on which we can run all the basic preprocessing methods. To list all the files in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_texts.fileids()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_texts.words('To Kill A Mockingbird.txt')  # uses punkt tokenizer like above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_texts.sents('To Kill A Mockingbird.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also add as paragraph method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_texts.paras('To Kill A Mockingbird.txt')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save these to a variable to look at the next step on a low level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_sents = my_texts.sents('To Kill A Mockingbird.txt')\n",
    "print (m_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a corpus, or text, from which we can get any of the statistics you learned in Day 3 of the Python workshop. We will review some of these functions once we get some more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) POS-Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many situations, in which \"tagging\" words (or really anything) may be useful in order to determine or calculate trends, or for further text analysis to extract meaning. NLTK contains several methods to achieve this, from simple regex to more advanced machine learning models models.\n",
    "\n",
    "It is important to note that in Natural Language Processing (NLP), POS (Part of Speech) tagging is the most common use for tagging, but the actual tag can be anything. Other applications include sentiment analysis and NER (Named Entity Recognition). Tagging is simply labeling a word to a specific category via a tuple.\n",
    "\n",
    "Nevertheless, for training more advanced tagging models, POS tagging is nearly essential. If you are defining a machine learning model to predict patterns in your text, these patterns will most likley rely on, among other things, POS features. You will therefore first tag POS and then use the POS as a feature in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On a low-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagging is creating a tuple of (word, tag) for every word in a text or corpus. For example: \"My name is Chris\" may be tagged for POS as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My/PossessivePronoun name/Noun is/Verb Chris/ProperNoun ./Period\n",
    "\n",
    "*NB: type 'nltk.data.path' to find the path on your computer to your downloaded nltk corpora. You can explore these files to see how large corpora are formatted.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice how the text is annotated, using a forward slash to match the word to its tag. So how can we get this to a useful form for Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag import str2tuple\n",
    "\n",
    "line = \"My/Possessive_Pronoun name/Noun is/Verb Chris/Proper_Noun ./Period\"\n",
    "tagged_sent = [str2tuple(t) for t in line.split()]\n",
    "\n",
    "print (tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further analysis of tags with NLTK requires a *list* of sentences, otherwise you will get an index error on higher level methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, these tags are a bit verbose, the standard tagging conventions follow the Penn Treebank (more in a second): https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's stock English `pos_tag` tagger is a perceptron tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "m_tagged_sent = pos_tag(m_sents[0])\n",
    "print (m_tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these tags mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import help\n",
    "help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_tagged_all = [pos_tag(sent) for sent in m_sents]\n",
    "print(m_tagged_all[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find and aggregate certain parts of speech too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ConditionalFreqDist\n",
    "def find_tags(tag_prefix, tagged_text):\n",
    "    cfd = ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
    "                                  if tag.startswith(tag_prefix))\n",
    "    return dict((tag, cfd[tag].most_common(5)) for tag in cfd.conditions()) #cfd.conditions() yields all tags possibilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_tagged_words = [item for sublist in m_tagged_all for item in sublist]\n",
    "\n",
    "tagdict = find_tags('JJ', m_tagged_words)\n",
    "for tag in sorted(tagdict):\n",
    "    print(tag, tagdict[tag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can begin to quantify syntax by look at environments of words, so what commonly follows a verb?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = [b[1] for (a, b) in nltk.bigrams(m_tagged_words) if a[1].startswith('VB')]\n",
    "fd1 = nltk.FreqDist(tags)\n",
    "\n",
    "print (\"To Kill A Mockingbird\")\n",
    "fd1.tabulate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a tagged corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how tagging works, we can quickly tag all of our documents, but we'll only do a few hundred from the much larger corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sents = {}\n",
    "for fid in my_texts.fileids()[::10]:\n",
    "    tagged_sents[fid.split(\".\")[0]] = [pos_tag(sent) for sent in my_texts.sents(fid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sents[\"Harry Potter and the Prisoner of Azkaban\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute frequencies are available through NLTK's `FreqDist` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_tags = []\n",
    "all_tups = []\n",
    "\n",
    "for k in tagged_sents.keys():\n",
    "    for s in tagged_sents[k]:\n",
    "        for t in s:\n",
    "            all_tags.append(t[1])\n",
    "            all_tups.append(t)\n",
    "\n",
    "nltk.FreqDist(all_tags).tabulate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = ['NN', 'VB', 'JJ']\n",
    "for t in tags:\n",
    "    tagdict = find_tags(t, all_tups)\n",
    "    for tag in sorted(tagdict):\n",
    "        print(tag, tagdict[tag])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this to other genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "for c in brown.categories():\n",
    "    tagged_words = brown.tagged_words(categories=c) #not universal tagset\n",
    "    tag_fd = nltk.FreqDist(tag for (word, tag) in tagged_words)\n",
    "    print(c.upper())\n",
    "    tag_fd.tabulate(10)\n",
    "    print()\n",
    "    tags = ['NN', 'VB', 'JJ']\n",
    "    for t in tags:\n",
    "        tagdict = find_tags(t, tagged_words)\n",
    "        for tag in sorted(tagdict):\n",
    "            print(tag, tagdict[tag])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at what linguistic environment words are in on a low level, below lists all the words preceding \"love\" in the romance category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "brown_news_text = brown.words(categories='romance')\n",
    "sorted(set(a for (a, b) in nltk.bigrams(brown_news_text) if b == 'love'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While tagging parts of speech can be helpful for certain NLP tasks, dependency parsing is better at extracting real relationships within a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar = \"/Users/chench/Documents/stanford-parser-full-2015-12-09/stanford-parser.jar\",\n",
    "                                             path_to_models_jar = \"/Users/chench/Documents/stanford-parser-full-2015-12-09/stanford-parser-3.6.0-models.jar\")\n",
    "\n",
    "result = dependency_parser.raw_parse_sents(['I shot an elephant in my sleep.', 'It was great.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the program takes longer to run, I will not run it on the entire corpus, but an example is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for r in result:\n",
    "    for o in r:\n",
    "        trips = list(o.triples())  # ((head word, head tag), rel, (dep word, dep tag))\n",
    "        for t in trips:\n",
    "            print(t)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokening, tagging, and parser, one of the last steps in the pipeline is NER. Identifying named entities can be useful in determing many different relationships, and often serves as a prerequisite to mapping textual relationships within a set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "\n",
    "ner_tag = StanfordNERTagger(\n",
    "        '/Users/chench/Documents/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "        '/Users/chench/Documents/stanford-ner-2015-12-09/stanford-ner.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyprind\n",
    "\n",
    "ner_sents = {}\n",
    "books = [\"To Kill A Mockingbird.txt\", \"Harry Potter and the Prisoner of Azkaban.txt\"]\n",
    "\n",
    "for fid in books:\n",
    "    bar = pyprind.ProgBar(len(my_texts.sents(fid)), monitor=True, bar_char=\"#\")\n",
    "    tagged_sents = []\n",
    "    for sent in my_texts.sents(fid):\n",
    "        tagged_sents.append(ner_tag.tag(sent))\n",
    "        bar.update()\n",
    "    ner_sents[fid.split(\".\")[0]] = tagged_sents\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look on the low level at a single summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ner_sents[\"To Kill A Mockingbird\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from nltk import FreqDist\n",
    "\n",
    "NER = {\"LOCATION\": [],\n",
    "       \"PERSON\": [],\n",
    "       \"ORGANIZATION\": [],\n",
    "       }\n",
    "\n",
    "for sentence in ner_sents[\"To Kill A Mockingbird\"]:\n",
    "    for tag, chunk in groupby(sentence, lambda x: x[1]):\n",
    "        if tag != \"O\":\n",
    "            NER[tag].append(\" \".join(w for w, t in chunk))\n",
    "\n",
    "if NER[\"LOCATION\"]:\n",
    "    print(\"Locations:\")\n",
    "    FreqDist(NER[\"LOCATION\"]).tabulate()\n",
    "    print()\n",
    "\n",
    "if NER[\"PERSON\"]:\n",
    "    print(\"Persons:\")\n",
    "    FreqDist(NER[\"PERSON\"]).tabulate()\n",
    "    print()\n",
    "\n",
    "if NER[\"ORGANIZATION\"]:\n",
    "    print(\"Organizations\")\n",
    "    FreqDist(NER[\"ORGANIZATION\"]).tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or between the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NER = {\"LOCATION\": [],\n",
    "       \"PERSON\": [],\n",
    "       \"ORGANIZATION\": [],\n",
    "       }\n",
    "\n",
    "for k in ner_sents.keys():\n",
    "    for sentence in ner_sents[k]:\n",
    "        for tag, chunk in groupby(sentence, lambda x: x[1]):\n",
    "            if tag != \"O\":\n",
    "                NER[tag].append(\" \".join(w for w, t in chunk))\n",
    "\n",
    "if NER[\"LOCATION\"]:\n",
    "    print(\"Locations:\")\n",
    "    FreqDist(NER[\"LOCATION\"]).tabulate()\n",
    "    print()\n",
    "\n",
    "if NER[\"PERSON\"]:\n",
    "    print(\"Persons:\")\n",
    "    FreqDist(NER[\"PERSON\"]).tabulate()\n",
    "    print()\n",
    "\n",
    "if NER[\"ORGANIZATION\"]:\n",
    "    FreqDist(NER[\"ORGANIZATION\"]).tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While earlier sentiment analysis was based on simple dictionary look-up methods denoting words as positive or negative, or assigning numerical values to words, newer methods are better able to take a word's or sentence's environment into account. VADER (Valence Aware Dictionary and sEntiment Reasoner) is one such example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(sid.polarity_scores(\"I really don't like that book.\")[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fid in books:\n",
    "    print(fid.upper())\n",
    "    sent_pols = [sid.polarity_scores(s)[\"compound\"] for s in sent_tokenize(my_texts.raw(fid))]\n",
    "    for i, s in enumerate(my_texts.sents(fid)):\n",
    "        print(s, sent_pols[i])\n",
    "        print()\n",
    "    \n",
    "    print()\n",
    "    print(\"Mean: \", np.mean(sent_pols))\n",
    "    print()\n",
    "    print(\"=\"*100)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
